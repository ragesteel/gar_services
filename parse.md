# Получение, импорт, актуализация данных ГАР

NB разумеется пока это похоже больше на эксперименты со Spring'ом, ибо всё ещё находиться в состоянии разработки. 

## Получение архивов выгрузок

Для обработки адресных данных нужно сначала получить их выгрузку. Можно ли самостоятельно пойти [скачать файлы](https://fias.nalog.ru/Frontend) или воспользоваться веб-сервисом, который отдаст нужный URL. А вот каких-либо средств для скачивания файла тут нет. Полный gar_xml на начало осени 2025-го весит 47 Гб, т.е. даже в идеальном случае на интернете со скорость 50 Мбит/с будет скачиваться больше двух часов. Хорошо что помимо полного архива есть и дельта.

### Работа с данными выгрузок

Чтобы не распаковывать архив (и не превращать архив на 47 Гб в отдельные файлы общим объёмом 361 Гб) сделан класс для обработки архива — `GarZipFile`. 

Все поддерживаем разновидности элементов находятся в пакете `ru.gt2.gar.parser.domain`. Для запуска обработки нужно создать два объекта — `XMLStreamProcessor` одним из его статических методов и обработчик — экземпляр `Consumer`, куда в процессе будут отправлять списки записей в процессе их обработки. 

## Источник вдохновения

На Хабр была статья Евгения Ляшева [Универсальный загрузчик XML на java. Или как загрузить файлы ГАР на 250 гб и остаться при памяти](https://habr.com/ru/articles/724324/). Помимо кода в самой статье, есть ещё и [проект `XmlReaderGar` на github](https://github.com/lyashov/XmlReaderGar). 

## Разработчику на заметку

Для организации запускаемых классов с контекстом Spring — смотреть в сторону интерфейса CommandLineRunner.

Доклад с JPoint: [Многопоточная вставка данных в БД](https://www.youtube.com/watch?v=60qHJSnfhmM). Прямо сейчас это не нужно, пусть пока побудет тут.

## Возможные доработки

### Микросервис для скачивания архивов выгрузок.
У него минимальная БД — там только таблица с выгрузками, которая актуализируется по данным с сервиса и таблица с информацией о скачивании файлов. Так он по идее должен по планировщику ходить и обновлять информацию о загрузках и принимать запросы на начало загрузки. А по окончании загрузки он мог-бы событие отправлять.

## Вехи разработки

### 2025-09-02 15:13 MSK 

Проект должен был называться gar-import, но import — это ключевое слово в java, а писать imprt в имени пакета не хотелось, по-этому пусть будет parse!

### 2025-09-02 16:35 MSK 

Среди всего многообразия code-review по `XMLAttributeReader` в [комментариях на хабре](https://habr.com/ru/articles/724324/comments/) никто не сказал что можно сделать его реализующим интерфейс `Iterable<List<T>` чтобы потом можно было со `Steam`'ами играться.

### 2025-09-03 23:23 MSK

Однако `ADM_HIERARCHY` уже прям напрашивается на многопоточную обработку. Но это не самое больше в обработке. Суммарный объем распакованных файлов — больше 38 гигов.

Сейчас вот уже 10 минут прошло. И непонятно сколько ещё будет… однако больше 13 минут. Но это на 143 905 004 элементов. И пока всё без ошибок.

### 2025-09-04 13:10 MSK

Всё-таки классная штука, этот ваш AI, сейчас Cursor мне буквально за 10 минут переписал комментарии которые я писал к полям record'а в виде `@param`ов основного комментария!

### 2025-09-04 13:35 MSK

Добавление типов с Cursor'ом тоже происходит быстрее!

### 2025-09-05 09:54 MSK

Начал было вести раздел «Подобные проекты» в [основном файле](README.md) как почти сразу стало понятно, что нужно заводить отдельный файлик с таблицей по таким проектам. Даже для того, что есть на github.

### 2025-09-05 11:05 MSK

Добавил `ChangeHistory`, ещё одну сущность с большим объёмом данных. Судя по тому что оно успешно идёт, особых проблем там не наблюдается.

### 2025-09-05 14:54 MSK

Однако почти 20 минут на чтение 427 343 921 записей и это безо всякой обработки.

### 2025-09-05 16:26 MSK

Ещё один из длительных процессов — добавил HOUSES. Теперь полностью с помощью Cursor'а, ткнул его в схему и он прям всё сделал. Мне только осталось немного одну строчку в другое место поставить.
Интересно сколько он будет HOUSES читать.

### 2025-09-05 16:37 MSK 

Однако HOUSE там 78 775 598 записей, которые он обработал за 9 минут.

Дальше тут сказал Cursor'у сделать мне MUN_HIERARCHY. Он всё хорошо сделал, кроме одного — поля ОКТМО. Там в схеме указано два ограничения — длина от 1 до 11 и регулярка `[0-9]{8,11}` и согласно ей нужно было указать длину от 8 до 11. Сказал ему исправить — он не осилил. Всё-таки этот AI он пока ещё джун. Ну или по крайней мере бесплатная версия.

### 2025-09-05 17:14 MSK

MUN_HIERARCHY — 146 220 247 записей, обработка - больше 12 минут.

Добавил NORMATIVE_DOCS, NORMATIVE_DOCS_KINDS, NORMATIVE_DOCS_TYPES. Сначала он начал немного придумывать, но потом я всё-таки уговорил Cursor создавать record'ы по схемам. Впрочем, всё равно не доглядел — он взял к именам полей, которые являются ссылками добавил суффикс «id». Но это хоть сразу выясняется.

Всего записей в NORMATIVE_DOCS — 19 671 819, время обработки — чуть меньше двух минут.

### 2025-09-05 22:08 MSK

Хотел-было поиграть с Cursor'ом в рефакторинг, а он уже говорит, мол, лимит, хотя потрачено всего-то 73.2 %. Видимо самое время попробовать GigaIDE…

Судя по всему, я могу поставить GigaCode и в OpenIDE… Да, я могу. Но приходится ставить вручную, а не из магазина.

Но он не хочет решать то что я хочу от него получить по запросу: Замени вызовы конструкторов коллекций вызовами статистических методов из guava. Если включить Think, то да, он что-то выдаст, но блин, оно в таком неудобном виде, что прям даже не знаю.

Ладно, может он сможет мне сгенерировать record из XSD'шки… Мда-уж, может судя по всему он вообще не считал схему. И javadoc добавлял к самим полям record'а, а не `@param` класса, прям как я. А поля да — гигачат сам придумал. Указал ему на схему. Уже лучше, но опять-же javadoc параметров почему-то снизу. 

### 2025-09-06 22:58 MSK

Вроде добавил один только Param, но заодно ещё пять разновидностей файлов прочитал. Вот оно за 8 минут обработалось.
ADDR_OBJ_PARAMS    25 899 697
HOUSES_PARAMS     521 419 707
APARTMENTS_PARAMS 122 404 833
ROOMS_PARAMS        1 163 131
STEADS_PARAMS     281 113 771
CARPLACES_PARAMS      670 416

Комментарии переформатировал. И подумал что может ещё нужно какие-нибудь Record'ы слить.

### 2025-09-06 23:40 MSK

REESTR_OBJECTS 124 033 434 — 7 минут где-то.

### 2025-09-06 23:57 MSK 

А между тем остаётся всего 3 разновидности файлов. Нужно просто заканчивать и всё.

### 2025-09-07 11:46 MSK

Теперь все виды файлов разбираю, доделал последние три. Дальше дело за статистикой.

### 2025-09-08 00:23 MSK

Вроде-бы всего-то просто решил добавить более корректную обработку @Nullable — в итоге нашёл пару проблем в Record'ах. Было несколько optional понял которые были представлены примитивными типам. Зато теперь и Null правильно обрабатывается и вообще статистика симпатичней будет.

### 2025-09-08 07:36 MSK

Просто обработка XML — больше 2.5 часов, Stopwatch насчитал 2.56 часа.
Интересно, насколько можно ускорить за счёт многопоточности…

### 2025-09-08 15:54 MSK

Однако аннотации и record'ы — тот ещё котельчик! jspecify.Nullable не получить из RecordComponent. springframework.lang.Nullable — аналогично, правда может его можно получить из поля или метода, это я уже не проверял. Решил лучше быть завязанным на Jakarta Annotation, оттуда Nullable остаётся в RecordComponent. По крайней мере это лучше, чем заводить свою собственную.

### 2025-09-08 23:07 MSK

Попробовал-было сделать так, чтобы можно было складывать результаты статистики, но внезапно оказалось, что я не могу складывать Stopwatch. А методы, которые генерируют строчку, находятся глубоко внутри Guava. А чего-то подобного сходу не находятся. Да есть, `PrettyTime` и `ThreeTen-extras`, но сходу не понятно, как это использовать. Хоть бери, да выковыривай из Guava.

Да и нужно понять как мне хочется выводить время. Пока мне не нравится в Guava 8.106 min и 