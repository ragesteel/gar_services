# Получение, импорт, актуализация данных ГАР

NB разумеется пока это похоже больше на эксперименты со Spring'ом, ибо всё ещё находиться в состоянии разработки. 

## Получение архивов выгрузок

Для обработки адресных данных нужно сначала получить их выгрузку. Можно ли самостоятельно пойти [скачать файлы](https://fias.nalog.ru/Frontend) или воспользоваться веб-сервисом, который отдаст нужный URL. А вот каких-либо средств для скачивания файла тут нет. Полный gar_xml на начало осени 2025-го весит 47 Гб, т.е. даже в идеальном случае на интернете со скорость 50 Мбит/с будет скачиваться больше двух часов. Хорошо что помимо полного архива есть и дельта.

## Работа с данными выгрузок

Чтобы не распаковывать архив (и не превращать архив на 47 Гб в отдельные файлы общим объёмом 361 Гб) сделан класс для обработки архива — `GarZipFile`. 

Все поддерживаем разновидности элементов находятся в пакете `ru.gt2.gar.parser.domain`. Для запуска обработки нужно создать два объекта — `XMLStreamProcessor` одним из его статических методов и обработчик — экземпляр `Consumer`, куда в процессе будут отправлять списки записей в процессе их обработки. 

## Обработка полученных XML'ок

### Источник вдохновения

На Хабр была статья Евгения Ляшева [Универсальный загрузчик XML на java. Или как загрузить файлы ГАР на 250 гб и остаться при памяти](https://habr.com/ru/articles/724324/). Помимо кода в самой статье, есть ещё и [проект `XmlReaderGar` на github](https://github.com/lyashov/XmlReaderGar). 

## Загрузка данных в SQL-базу

Сначала нужно создать схемы данных. И тут хочется не ручками писать создание схем, а сгенерировать их. Вернее даже не просто сгенерировать схемы, а сгенерировать описание таблицы в формате для Liquibase. 

Судя по заметке [вставка миллионов строк в java, стратегии и производительность](https://blog.tarkalabs.com/inserting-millions-of-records-in-java-strategies-and-benchmarks-475cbb9c02ca), самый быстрый способ — это воспользоваться [командой COPY](https://www.postgresql.org/docs/current/sql-copy.html). В идеале не просто CSV туда слать, а использовать бинарный формат. Для этого даже есть проект [PgBulkInsert](https://github.com/bytefish/PgBulkInsert).

Ещё можно использовать [UNNEST для пакетных вставок](https://ecode.ozon.tech/talks/e73471bf364a43ba8dd353bbf9a0e1c7/?referer=%2Ftalks%2Ff22529885e974f188e5c97c18eca2603%2F%23s), вот текстовая статейка [по аналогичной тематике](https://www.tigerdata.com/blog/boosting-postgres-insert-performance).

## Разработчику на заметку

Для организации запускаемых классов с контекстом Spring — смотреть в сторону интерфейса CommandLineRunner.

Доклад с JPoint: [Многопоточная вставка данных в БД](https://www.youtube.com/watch?v=60qHJSnfhmM). Прямо сейчас это не нужно, пусть пока побудет тут.

[Генератор banner.txt](https://devops.datenkollektiv.de/banner.txt/index.html), шрифты которые мне глянулись: standard, big, slant. А вот насчёт цветов — первая ссылка в google — на Baeldung.

## Возможные доработки

### Микросервис для скачивания архивов выгрузок.
У него минимальная БД — там только таблица с выгрузками, которая актуализируется по данным с сервиса и таблица с информацией о скачивании файлов. Так он по идее должен по планировщику ходить и обновлять информацию о загрузках и принимать запросы на начало загрузки. А по окончании загрузки он мог-бы событие отправлять.

## Вехи разработки

### 2025-09-02 15:13 MSK 

Проект должен был называться gar-import, но import — это ключевое слово в java, а писать imprt в имени пакета не хотелось, по-этому пусть будет parse!

### 2025-09-02 16:35 MSK 

Среди всего многообразия code-review по `XMLAttributeReader` в [комментариях на хабре](https://habr.com/ru/articles/724324/comments/) никто не сказал что можно сделать его реализующим интерфейс `Iterable<List<T>` чтобы потом можно было со `Steam`'ами играться.

### 2025-09-03 23:23 MSK

Однако `ADM_HIERARCHY` уже прям напрашивается на многопоточную обработку. Но это не самое больше в обработке. Суммарный объем распакованных файлов — больше 38 гигов.

Сейчас вот уже 10 минут прошло. И непонятно сколько ещё будет… однако больше 13 минут. Но это на 143 905 004 элементов. И пока всё без ошибок.

### 2025-09-04 13:10 MSK

Всё-таки классная штука, этот ваш AI, сейчас Cursor мне буквально за 10 минут переписал комментарии которые я писал к полям record'а в виде `@param`ов основного комментария!

### 2025-09-04 13:35 MSK

Добавление типов с Cursor'ом тоже происходит быстрее!

### 2025-09-05 09:54 MSK

Начал было вести раздел «Подобные проекты» в [основном файле](README.md) как почти сразу стало понятно, что нужно заводить отдельный файлик с таблицей по таким проектам. Даже для того, что есть на github.

### 2025-09-05 11:05 MSK

Добавил `ChangeHistory`, ещё одну сущность с большим объёмом данных. Судя по тому что оно успешно идёт, особых проблем там не наблюдается.

### 2025-09-05 14:54 MSK

Однако почти 20 минут на чтение 427 343 921 записей и это безо всякой обработки.

### 2025-09-05 16:26 MSK

Ещё один из длительных процессов — добавил HOUSES. Теперь полностью с помощью Cursor'а, ткнул его в схему и он прям всё сделал. Мне только осталось немного одну строчку в другое место поставить.
Интересно сколько он будет HOUSES читать.

### 2025-09-05 16:37 MSK 

Однако HOUSE там 78 775 598 записей, которые он обработал за 9 минут.

Дальше тут сказал Cursor'у сделать мне MUN_HIERARCHY. Он всё хорошо сделал, кроме одного — поля ОКТМО. Там в схеме указано два ограничения — длина от 1 до 11 и регулярка `[0-9]{8,11}` и согласно ей нужно было указать длину от 8 до 11. Сказал ему исправить — он не осилил. Всё-таки этот AI он пока ещё джун. Ну или по крайней мере бесплатная версия.

### 2025-09-05 17:14 MSK

MUN_HIERARCHY — 146 220 247 записей, обработка - больше 12 минут.

Добавил NORMATIVE_DOCS, NORMATIVE_DOCS_KINDS, NORMATIVE_DOCS_TYPES. Сначала он начал немного придумывать, но потом я всё-таки уговорил Cursor создавать record'ы по схемам. Впрочем, всё равно не доглядел — он взял к именам полей, которые являются ссылками добавил суффикс «id». Но это хоть сразу выясняется.

Всего записей в NORMATIVE_DOCS — 19 671 819, время обработки — чуть меньше двух минут.

### 2025-09-05 22:08 MSK

Хотел-было поиграть с Cursor'ом в рефакторинг, а он уже говорит, мол, лимит, хотя потрачено всего-то 73.2 %. Видимо самое время попробовать GigaIDE…

Судя по всему, я могу поставить GigaCode и в OpenIDE… Да, я могу. Но приходится ставить вручную, а не из магазина.

Но он не хочет решать то что я хочу от него получить по запросу: Замени вызовы конструкторов коллекций вызовами статистических методов из guava. Если включить Think, то да, он что-то выдаст, но блин, оно в таком неудобном виде, что прям даже не знаю.

Ладно, может он сможет мне сгенерировать record из XSD'шки… Мда-уж, может судя по всему он вообще не считал схему. И javadoc добавлял к самим полям record'а, а не `@param` класса, прям как я. А поля да — гигачат сам придумал. Указал ему на схему. Уже лучше, но опять-же javadoc параметров почему-то снизу. 

### 2025-09-06 22:58 MSK

Вроде добавил один только Param, но заодно ещё пять разновидностей файлов прочитал. Вот оно за 8 минут обработалось.
ADDR_OBJ_PARAMS    25 899 697
HOUSES_PARAMS     521 419 707
APARTMENTS_PARAMS 122 404 833
ROOMS_PARAMS        1 163 131
STEADS_PARAMS     281 113 771
CARPLACES_PARAMS      670 416

Комментарии переформатировал. И подумал что может ещё нужно какие-нибудь Record'ы слить.

### 2025-09-06 23:40 MSK

REESTR_OBJECTS 124 033 434 — 7 минут где-то.

### 2025-09-06 23:57 MSK 

А между тем остаётся всего 3 разновидности файлов. Нужно просто заканчивать и всё.

### 2025-09-07 11:46 MSK

Теперь все виды файлов разбираю, доделал последние три. Дальше дело за статистикой.

### 2025-09-08 00:23 MSK

Вроде-бы всего-то просто решил добавить более корректную обработку @Nullable — в итоге нашёл пару проблем в Record'ах. Было несколько optional понял которые были представлены примитивными типам. Зато теперь и Null правильно обрабатывается и вообще статистика симпатичней будет.

### 2025-09-08 07:36 MSK

Просто обработка XML — больше 2.5 часов, Stopwatch насчитал 2.56 часа.
Интересно, насколько можно ускорить за счёт многопоточности…

### 2025-09-08 15:54 MSK

Однако аннотации и record'ы — тот ещё котельчик! jspecify.Nullable не получить из RecordComponent. springframework.lang.Nullable — аналогично, правда может его можно получить из поля или метода, это я уже не проверял. Решил лучше быть завязанным на Jakarta Annotation, оттуда Nullable остаётся в RecordComponent. По крайней мере это лучше, чем заводить свою собственную.

### 2025-09-08 23:07 MSK

Попробовал-было сделать так, чтобы можно было складывать результаты статистики, но внезапно оказалось, что я не могу складывать Stopwatch. А методы, которые генерируют строчку, находятся глубоко внутри Guava. А чего-то подобного сходу не находятся. Да есть, `PrettyTime` и `ThreeTen-extras`, но сходу не понятно, как это использовать. Хоть бери, да выковыривай из Guava.

Да и нужно понять как мне хочется выводить время. Пока мне не нравится в Guava 8.106 min и 2.506 h. Но и просто выводить 22:54 — тоже не вариант, ведь не понятно то чч:мм или мм:сс.

Даже в Spring есть DurationStyle.SIMPLE, но опять-же там нет автовыбора.

### 2025-09-08 23:57 MSK

Даже нужно думать о сохранении данных в базу. Но сначала нужно создать схему данных.

Какой-нибудь простейший генератор из объектов пакета `gar.parse.domain`. Первое поле — всегда идентификатор.

А потом Spring Data JDBC и R2DBC, потом и сравнивать их!

### 2025-09-11 23:29 MSK

Итак, описание схемы для Liquibase. Первое, что хочется — это добавить описание источника в какую-нибудь аннотацию Record'а, чтобы из этого генерировать id changeset'а.
Дальше хочется ещё колонок с комментариями как для таблиц, так и для колонок.

### 2025-09-12 14:33 MSK

Оказывается у меня `mvn clean install` не проходит. Жалуется на Execution repackage failed: Unable to find main class. И первые найденные в интернете советы не помогают.

### 2025-09-13 19:41 MSK

Пришлось поубирать generic'и для упрощения обработки. Пока последовательной, но с расчётом на то, что это можно будет запаралелить.

### 2025-09-14 21:29 MSK

Наконец-то я увидел 100% загрузку процессора. Спустя 6 минут после начала работы, как диск отпустило. Видимо исходные данные стоит всё-таки на SSD хранить, просто для скорости. Да и вообще оно хорошо когда исходные данные на одном диске, а запись уже на другой идёт.

Спустя 9 минут опять в диск упёрлись и загрузка процессора снизилась. Не, ну тут определённо нужно на системный SSD перенести данные. Только после окончания прогона, чтобы хоть какие-то адекватные цифры получить.

А сейчас вообще загрузка процессора до 30% упала. При этом диск на 74% занят, скорость чтения — около 180 Мб/с по Task Manager'у.

### 2025-09-14 22:51 MSK

Ну что, с SSD загрузка повыше, но не всегда прям 100%. 80-90% тоже бывает.

### 2025-09-15 08:32 MSK

Да, перенос на SSD даёт выигрыш, но не такой большой. 30 минут против 38-ми. Можно конечно ещё поиграться с режимами работы процессора, но это уже такое.

### 2025-09-19 13:52 MSK

Однако даже в таком простом приложении, нельзя просто так сделать переход Java 21 → 25. Всплыла ошибка: `Message: JAXP00010003: The length of entity "[xml]" is "100 001" that exceeds the "100 000" limit set by "jdk.xml.maxGeneralEntitySizeLimit"`.
Хорошо хоть нужное значение этого параметра можно задать в параметрах Factory перед созданием экземпляра. 
