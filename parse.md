# Получение, импорт, актуализация данных ГАР

NB разумеется пока это похоже больше на эксперименты со Spring'ом, ибо всё ещё находиться в состоянии разработки. 

## Получение архивов выгрузок

Для обработки адресных данных нужно сначала получить их выгрузку. Можно ли самостоятельно пойти [скачать файлы](https://fias.nalog.ru/Frontend) или воспользоваться веб-сервисом, который отдаст нужный URL. А вот каких-либо средств для скачивания файла тут нет. Полный gar_xml на начало осени 2025-го весит 47 Гб, т.е. даже в идеальном случае на интернете со скорость 50 Мбит/с будет скачиваться больше двух часов. Хорошо что помимо полного архива есть и дельта.

### Работа с данными выгрузок

Чтобы не распаковывать архив (и не превращать архив на 47 Гб в отдельные файлы общим объёмом 361 Гб) сделан класс для обработки архива — `GarZipFile`. 

Все поддерживаем разновидности элементов находятся в пакете `ru.gt2.gar.parser.domain`. Для запуска обработки нужно создать два объекта — `XMLStreamProcessor` одним из его статических методов и обработчик — экземпляр `Consumer`, куда в процессе будут отправлять списки записей в процессе их обработки. 

## Источник вдохновения

На Хабр была статья Евгения Ляшева [Универсальный загрузчик XML на java. Или как загрузить файлы ГАР на 250 гб и остаться при памяти](https://habr.com/ru/articles/724324/). Помимо кода в самой статье, есть ещё и [проект `XmlReaderGar` на github](https://github.com/lyashov/XmlReaderGar). 

## Разработчику на заметку

Для организации запускаемых классов с контекстом Spring — смотреть в сторону интерфейса CommandLineRunner.

Доклад с JPoint: [Многопоточная вставка данных в БД](https://www.youtube.com/watch?v=60qHJSnfhmM). Прямо сейчас это не нужно, пусть пока побудет тут.

## Возможные доработки

### Микросервис для скачивания архивов выгрузок.
У него минимальная БД — там только таблица с выгрузками, которая актуализируется по данным с сервиса и таблица с информацией о скачивании файлов. Так он по идее должен по планировщику ходить и обновлять информацию о загрузках и принимать запросы на начало загрузки. А по окончании загрузки он мог-бы событие отправлять.

## Вехи разработки

### 2025-09-02 15:13 MSK 

Проект должен был называться gar-import, но import — это ключевое слово в java, а писать imprt в имени пакета не хотелось, по-этому пусть будет parse!

### 2025-09-02 16:35 MSK 

Среди всего многообразия code-review по `XMLAttributeReader` в [комментариях на хабре](https://habr.com/ru/articles/724324/comments/) никто не сказал что можно сделать его реализующим интерфейс `Iterable<List<T>` чтобы потом можно было со `Steam`'ами играться.

### 2025-09-03 23:23 MSK

Однако `ADM_HIERARCHY` уже прям напрашивается на многопоточную обработку. Но это не самое больше в обработке. Суммарный объем распакованных файлов — больше 38 гигов.

Сейчас вот уже 10 минут прошло. И непонятно сколько ещё будет… однако больше 13 минут. Но это на 143 905 004 элементов. И пока всё без ошибок.

### 2025-09-04 13:10 MSK

Всё-таки классная штука, этот ваш AI, сейчас Cursor мне буквально за 10 минут переписал коментарии которые я писал к полям record'а в в виде `@param`ов основного комментария!

### 2025-09-04 13:35 MSK

Добавление типов с Cursor'ом тоже происходит быстрее!

### 2025-09-05 09:54 MSK

Начал было вести раздел «Подобные проекты» в [основном файле](README.md) как почти сразу стало понятно, что нужно заводить отдельный файлик с таблицей по таким проектам. Даже для того, что есть на github.

### 2025-09-05 11:05 MSK

Добавил `ChangeHistory`, ещё одну сущность с большим объёмом данных. Судя по тому что оно успешно идёт, особых проблем там не наблюдается.

### 2025-09-05 14:54 MSK

Однако почти 20 минут на чтение 427 343 921 записей и это безо всякой обработки.

### 2025-09-05 16:26 MSK

Ещё один из длительных процессов — добавил HOUSES. Теперь полностью с помощью Cursor'а, ткнул его в схему и он прям всё сделал. Мне только осталось немного одну строчку в другое место поставить.
Интересно сколько он будет HOUSES читать.

### 2025-09-05 16:37 MSK 

Однако HOUSE там 78 775 598 записей, которые он обработал за 9 минут.

Дальше тут сказал Cursor'у сделать мне MUN_HIERARCHY. Он всё хорошо сделал, кроме одного — поля ОКТМО. Там в схеме указано два ограничения — длина от 1 до 11 и регулярка `[0-9]{8,11}` и согласно ей нужно было указать длину от 8 до 11. Сказал ему исправить — он не осилил. Всё-таки этот AI он пока ещё джун. Ну или по крайней мере бесплатная версия.
